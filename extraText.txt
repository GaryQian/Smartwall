


Method for markerless hand tracking:
Our stretch goal for this project was to be able to recognize and track hands without having to rely on our green target markers. We were not able to implement this in time for our demo, however, we were able to eventually implement this functionality using a background subtractor coupled with a skin tone segmentation algorithm. 

The first step in our algorithm involved using a background subtractor to remove all static pixels from the video feed. This method does not infringe on other aspects of our design, as we already require our camera to be stationary during use. The background subtractor is set to "learn" relatively slowly, so as long as users are not completely stationary in front of the camera, the subtractor will be able to work as desired. 

The next step involves converting the remaining colors to YCrCb color space. We do this in order to better identify skin-tone colored regions from non skin-tone colored regions, and find contours around all skin-tone colored regions. We apply area constraints on these regions to rule out areas that are too large (such as skin-tone colored clothing) or small (backgrond noise) to be a hand. Finally, we removed regions that had extreme aspect ratios (long lines), as those were almost never hands. We also modified the background subtractor's learning rate based on the number of skintone regions found in the scene. Idealy, the background subtractor will have a low learning rate so that the user won't be subtracted out of the frame, but in the case that too many skin-tone regions were found, we would temporarily increase learning rate to remove static skin-tone regions from the frame.

Finally, we output the result of our program in real time. The following is one of our ideal outputs from our program.

//image here//

In order to integrate this with our system, we would have to add a third prediction for our deep learning model, which would be "not a hand". Getting training data for this would not be difficult, as we could easily collect thousands of 32x32 frames from our camera that are not hands. Once the model is re-trained, we would use the product as normal, and our model input would change from a 32x32 frame around the green target to a 32x32 frame around the center of detected contours from markerless hand tracker. It would only take action if it detected a single open or closed hand, otherwise, it would do nothing. Since our deep learning model has been extremely accurate up until now, we see no reason to believe it would not work after adding a third prediction.

One of the issues we faced with this method is that it requires the user to not wear short sleeves, since the skin regions detected would include the arm. To solve this, the user could wear a watch or thick bracelet to segment the hand from the arm. A more advanced solution would involve using a body segmentation algorithm that could extract the skeleton of a user on camera similar to those outputted by kinect sensors. However, this may not be very easy to implement using just a single webcam.

To conclude, we feel that our markerless hand tracking algorithm is a good foundation to begin tracking hands without markers, however, there are many edge cases faced in testing that would need to be controlled for such as short sleeves, skin colored clothing, shadows from strange lighting, and issues with users who stand still too long. However, in the face of these issues, we have achieved good control outputs in the perfect conditions.


